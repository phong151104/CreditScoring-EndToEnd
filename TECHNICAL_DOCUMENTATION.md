# Credit Scoring System - T√†i Li·ªáu K·ªπ Thu·∫≠t Chi Ti·∫øt

## M·ª•c L·ª•c
1. [T·ªïng Quan H·ªá Th·ªëng](#1-t·ªïng-quan-h·ªá-th·ªëng)
2. [Ki·∫øn Tr√∫c D·ª± √Ån](#2-ki·∫øn-tr√∫c-d·ª±-√°n)
3. [Frontend (Views)](#3-frontend-views)
4. [Backend Modules](#4-backend-modules)
5. [Thu·∫≠t To√°n Ch·∫•m ƒêi·ªÉm T√≠n D·ª•ng](#5-thu·∫≠t-to√°n-ch·∫•m-ƒëi·ªÉm-t√≠n-d·ª•ng)
6. [Quy Tr√¨nh X·ª≠ L√Ω D·ªØ Li·ªáu](#6-quy-tr√¨nh-x·ª≠-l√Ω-d·ªØ-li·ªáu)
7. [Session State Management](#7-session-state-management)
8. [T√≠ch H·ª£p AI/LLM](#8-t√≠ch-h·ª£p-aillm)
9. [H∆∞·ªõng D·∫´n C√†i ƒê·∫∑t & Ch·∫°y](#9-h∆∞·ªõng-d·∫´n-c√†i-ƒë·∫∑t--ch·∫°y)

---

## 1. T·ªïng Quan H·ªá Th·ªëng

### 1.1 M·ª•c ƒê√≠ch
H·ªá th·ªëng Credit Scoring l√† m·ªôt n·ªÅn t·∫£ng ƒë√°nh gi√° r·ªßi ro t√≠n d·ª•ng ho√†n ch·ªânh, cho ph√©p:
- Upload v√† ph√¢n t√≠ch d·ªØ li·ªáu kh√°ch h√†ng (EDA)
- X·ª≠ l√Ω d·ªØ li·ªáu (missing values, encoding, binning, balancing)
- Hu·∫•n luy·ªán nhi·ªÅu lo·∫°i model ML (Logistic Regression, Random Forest, XGBoost, LightGBM, CatBoost)
- Gi·∫£i th√≠ch model b·∫±ng SHAP values + AI interpretation
- D·ª± ƒëo√°n v√† t√≠nh ƒëi·ªÉm t√≠n d·ª•ng theo chu·∫©n ng√†nh

### 1.2 Technology Stack
| Component | Technology |
|-----------|------------|
| **Frontend** | Streamlit |
| **Backend** | Python, scikit-learn, XGBoost, LightGBM, CatBoost |
| **Explainability** | SHAP |
| **AI Integration** | Google Gemini API |
| **Data Processing** | Pandas, NumPy, imbalanced-learn |
| **Visualization** | Plotly |

---

## 2. Ki·∫øn Tr√∫c D·ª± √Ån

```
credit-scoring/
‚îú‚îÄ‚îÄ app.py                    # Entry point - Main router
‚îú‚îÄ‚îÄ views/                    # UI Pages (6 views)
‚îÇ   ‚îú‚îÄ‚îÄ home.py               # Dashboard + Workflow progress
‚îÇ   ‚îú‚îÄ‚îÄ upload_eda.py         # Data upload + EDA
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py # Feature processing (4131 lines)
‚îÇ   ‚îú‚îÄ‚îÄ model_training.py     # Training + Tuning
‚îÇ   ‚îú‚îÄ‚îÄ shap_explanation.py   # SHAP analysis + AI interpretation
‚îÇ   ‚îî‚îÄ‚îÄ prediction.py         # Single prediction + Credit score
‚îú‚îÄ‚îÄ backend/                  # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ models/               # ML models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py        # Training, CV, Hyperparameter tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictor.py      # Prediction + Credit score calculation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_importance.py
‚îÇ   ‚îú‚îÄ‚îÄ data_processing/      # Data transformations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encoder.py        # Categorical encoding (One-Hot, Label, Target, Frequency)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ balancer.py       # SMOTE, ADASYN, undersampling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ outlier_handler.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ explainability/       # SHAP
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shap_explainer.py
‚îÇ   ‚îî‚îÄ‚îÄ llm_integration/      # AI Analysis
‚îÇ       ‚îú‚îÄ‚îÄ config.py         # API keys config
‚îÇ       ‚îú‚îÄ‚îÄ eda_analyzer.py   # AI analysis for EDA
‚îÇ       ‚îî‚îÄ‚îÄ shap_analyzer.py  # AI interpretation for SHAP
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ session_state.py      # Streamlit session management
‚îÇ   ‚îî‚îÄ‚îÄ ui_components.py      # Reusable UI components
‚îî‚îÄ‚îÄ requirements.txt
```

---

## 3. Frontend (Views)

### 3.1 Navigation Flow
```mermaid
graph LR
    A[Dashboard] --> B[Upload & EDA]
    B --> C[Feature Engineering]
    C --> D[Model Training]
    D --> E[SHAP Explanation]
    E --> F[Prediction]
```

### 3.2 Chi Ti·∫øt T·ª´ng View

#### 3.2.1 Dashboard (`home.py`)
- Hi·ªÉn th·ªã workflow progress bar (8 b∆∞·ªõc)
- Th·ªëng k√™ session hi·ªán t·∫°i
- Quick stats v·ªÅ data v√† model

#### 3.2.2 Upload & EDA (`upload_eda.py`)
**Ch·ª©c nƒÉng:**
- Upload CSV files
- T·ª± ƒë·ªông detect data types
- EDA visualizations (histograms, box plots, correlation matrix)
- AI-powered EDA analysis (s·ª≠ d·ª•ng Gemini)

#### 3.2.3 Feature Engineering (`feature_engineering.py`)
**File l·ªõn nh·∫•t (4131 lines), bao g·ªìm:**

| Tab | Ch·ª©c nƒÉng |
|-----|-----------|
| **Ti·ªÅn X·ª≠ L√Ω** | Chia Train/Valid/Test, ch·ªçn target |
| **Missing Values** | Mean/Median/Mode/Forward Fill/Backward Fill |
| **Encoding** | One-Hot, Label, Target, Ordinal, Frequency |
| **Binning** | Equal Width, Equal Frequency, **Optimal Binning (WoE/IV)**, Custom |
| **Outliers** | IQR, Z-score, Winsorization |
| **Scaling** | StandardScaler, MinMaxScaler, RobustScaler |
| **Balancing** | SMOTE, ADASYN, Random Under/Over sampling |
| **Feature Selection** | Manual selection, correlation-based, importance-based |

##### Optimal Binning Algorithm (WoE/IV)
```python
# 1. S·ª≠ d·ª•ng Decision Tree ƒë·ªÉ t√¨m ƒëi·ªÉm c·∫Øt t·ªëi ∆∞u
tree = DecisionTreeClassifier(
    max_leaf_nodes=num_bins,
    min_samples_leaf=5% data  # M·ªói bin ‚â• 5%
)
tree.fit(X_column, y_target)

# 2. L·∫•y thresholds t·ª´ c√¢y
thresholds = tree.tree_.threshold
bins = [-inf] + thresholds + [inf]

# 3. T√≠nh WoE v√† IV
WoE = ln(% Good / % Bad)
IV = Œ£ (% Good - % Bad) √ó WoE
```

#### 3.2.4 Model Training (`model_training.py`)
**Supported Models:**
- Logistic Regression
- Random Forest
- XGBoost
- LightGBM
- CatBoost
- Gradient Boosting

**Training Options:**
- Standard Training
- Cross-Validation
- Hyperparameter Tuning (Grid Search, Random Search, Optuna)

#### 3.2.5 SHAP Explanation (`shap_explanation.py`)
- Global Feature Importance (mean |SHAP|)
- Summary Plot
- Force Plot cho t·ª´ng sample
- AI Interpretation (Gemini chat)

#### 3.2.6 Prediction (`prediction.py`)
- Single sample prediction
- Credit Score calculation
- Risk level assessment
- Recommendations

---

## 4. Backend Modules

### 4.1 Model Trainer (`backend/models/trainer.py`)

```python
def train_model(X_train, y_train, X_test, y_test, model_type, params=None):
    """
    Train model v√† tr·∫£ v·ªÅ (model, metrics)
    
    Returns:
        model: Trained sklearn/xgb/lgb/cb model
        metrics: {accuracy, precision, recall, f1, auc}
    """

def cross_validate_model(X, y, model_type, params=None, cv_folds=5):
    """K-fold Cross-validation v·ªõi stratified sampling"""

def hyperparameter_tuning(X, y, model_type, method, cv_folds=5, n_trials=50):
    """Grid Search, Random Search, Optuna (Bayesian)"""
```

### 4.2 Predictor (`backend/models/predictor.py`)

```python
def predict_single(model, input_data, feature_names, feature_stats=None):
    """
    D·ª± ƒëo√°n cho 1 sample, tr·∫£ v·ªÅ:
    - prediction: 0/1
    - probability: PD (probability of default)
    - credit_score: 300-850
    - risk_level: Very Low/Low/Medium/High/Very High
    - approval_status: approved/conditional/rejected
    - Recommendations
    """
```

### 4.3 Encoder (`backend/data_processing/encoder.py`)

```python
class CategoricalEncoder:
    def one_hot_encoding(data, columns, drop_first=False)
    def label_encoding(data, columns)
    def target_encoding(data, columns, target_column, smoothing=1.0)
    def ordinal_encoding(data, columns, ordinal_mappings=None)
    def frequency_encoding(data, columns)
    def recommend_encoding_method(data, column, target_column=None)
```

### 4.4 Balancer (`backend/data_processing/balancer.py`)

```python
def balance_data(data, target_column, method="SMOTE", **kwargs):
    """
    Methods: SMOTE, ADASYN, BorderlineSMOTE, 
             RandomOverSampler, RandomUnderSampler, 
             TomekLinks, SMOTEENN
    """
```

### 4.5 SHAP Explainer (`backend/explainability/shap_explainer.py`)

```python
class SHAPExplainer:
    def __init__(model, X_background, model_type):
        """
        Auto-select explainer:
        - TreeExplainer: RF, XGB, LGB, CB, GB
        - LinearExplainer: Logistic Regression
        - KernelExplainer: Fallback
        """
    
    def compute_shap_values(X)  # ‚Üí np.ndarray
    def get_feature_importance()  # ‚Üí DataFrame
    def get_local_explanation(sample_idx, X)  # ‚Üí Dict
```

---

## 5. Thu·∫≠t To√°n Ch·∫•m ƒêi·ªÉm T√≠n D·ª•ng

### 5.1 Credit Score Formula

> **Industry Standard: Log-Odds Scaling**

```python
# Constants
PDO = 30            # Points to Double Odds
base_score = 600    # Score at base odds
base_odds = 19      # Odds 19:1 = PD 5%

# Calculation
factor = PDO / ln(2)  # ‚âà 43.29
odds = (1 - PD) / PD
credit_score = base_score + factor √ó ln(odds / base_odds)

# Example:
# PD = 5%  ‚Üí  odds = 19  ‚Üí  Score = 600
# PD = 10% ‚Üí  odds = 9   ‚Üí  Score ‚âà 567
# PD = 2%  ‚Üí  odds = 49  ‚Üí  Score ‚âà 640
```

### 5.2 Risk Level Thresholds (5-tier)

| PD Range | Risk Level | M√†u |
|----------|------------|-----|
| < 2% | R·∫•t th·∫•p | üü¢ #10b981 |
| 2-5% | Th·∫•p | üü¢ #22c55e |
| 5-10% | Trung b√¨nh | üü† #f59e0b |
| 10-20% | Cao | üî¥ #ef4444 |
| > 20% | R·∫•t cao | üî¥ #dc2626 |

### 5.3 Score Interpretation (5-tier)

| Score Range | Interpretation |
|-------------|----------------|
| 750-850 | Xu·∫•t s·∫Øc |
| 650-749 | T·ªët |
| 550-649 | Trung b√¨nh |
| 450-549 | K√©m |
| 300-449 | R·∫•t k√©m |

### 5.4 Approval Decision Logic

```python
if PD < 5% AND score >= 650:
    ‚Üí "Ph√™ duy·ªát"
elif PD < 10% OR (550 <= score < 650):
    ‚Üí "C√≥ th·ªÉ xem x√©t v·ªõi ƒëi·ªÅu ki·ªán"
else:
    ‚Üí "T·ª´ ch·ªëi - R·ªßi ro cao"
```

---

## 6. Quy Tr√¨nh X·ª≠ L√Ω D·ªØ Li·ªáu

```mermaid
flowchart TD
    A[Upload CSV] --> B[Auto Detect Types]
    B --> C[EDA Analysis]
    C --> D[Handle Missing Values]
    D --> E[Encode Categorical]
    E --> F[Binning Continuous]
    F --> G[Handle Outliers]
    G --> H[Scaling]
    H --> I[Balance Classes]
    I --> J[Train/Valid/Test Split]
    J --> K[Feature Selection]
    K --> L[Training]
```

### 6.1 Data Flow trong Session State

```
st.session_state.data                  # Raw data
st.session_state.processed_data        # After preprocessing
st.session_state.train_data/valid_data/test_data  # Split data
st.session_state.selected_features     # Feature list for model
st.session_state.model                 # Trained model object
st.session_state.model_metrics         # {accuracy, precision, recall, f1, auc}
```

---

## 7. Session State Management

### 7.1 Key Session Variables

| Category | Variables |
|----------|-----------|
| **Data** | `data`, `processed_data`, `train_data`, `valid_data`, `test_data` |
| **Target** | `target_column` |
| **Model** | `model`, `model_type`, `selected_model_name`, `model_metrics` |
| **SHAP** | `shap_explainer_obj`, `shap_values_computed`, `shap_feature_importance` |
| **Configs** | `missing_config`, `encoding_config`, `binning_config`, `scaling_config` |
| **AI** | `ai_analysis`, `shap_chat_history` |

### 7.2 State Lifecycle

```python
# Initialize
init_session_state()  # Called on every page load

# Clear on new data upload
clear_data_related_state()  # Resets all model/SHAP/configs

# Check status
get_session_info()  # Returns {has_data, has_model, num_features, ...}
```

---

## 8. T√≠ch H·ª£p AI/LLM

### 8.1 Providers Supported
- **Google Gemini** (default): `gemini-2.5-flash`
- OpenAI GPT (optional)
- Anthropic Claude (optional)

### 8.2 AI Features

| Feature | Module | Description |
|---------|--------|-------------|
| EDA Analysis | `eda_analyzer.py` | Ph√¢n t√≠ch t·ª± ƒë·ªông data quality, recommendations |
| SHAP Interpretation | `shap_analyzer.py` | Gi·∫£i th√≠ch feature importance, local explanations |
| Chat | `shap_analyzer.py` | H·ªèi ƒë√°p v·ªÅ model |

### 8.3 Configuration

```bash
# .env file
GOOGLE_API_KEY=your_gemini_api_key
```

```python
# backend/llm_integration/config.py
class LLMConfig:
    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
    GOOGLE_MODEL = 'gemini-2.5-flash'
```

---

## 9. H∆∞·ªõng D·∫´n C√†i ƒê·∫∑t & Ch·∫°y

### 9.1 Requirements
```bash
pip install -r requirements.txt
```

**Key dependencies:**
- streamlit
- pandas, numpy
- scikit-learn
- xgboost, lightgbm, catboost
- shap
- imbalanced-learn
- plotly
- google-generativeai (optional)

### 9.2 Run Application
```bash
# Windows
RUN_APP.bat

# Linux/Mac
./RUN_APP.sh

# Or directly
streamlit run app.py
```

### 9.3 Sample Data
Located in `sample_data/` folder with example credit datasets.

---

## Appendix: API Reference

### Trainer API
```python
from backend.models.trainer import train_model, cross_validate_model, hyperparameter_tuning

model, metrics = train_model(X_train, y_train, X_test, y_test, "XGBoost", params)
cv_results = cross_validate_model(X, y, "Random Forest", params, cv_folds=5)
tuning_results = hyperparameter_tuning(X, y, "LightGBM", method="Optuna", n_trials=100)
```

### Predictor API
```python
from backend.models.predictor import predict_single, predict_batch

result = predict_single(model, input_dict, feature_names)
# result = {
#     'prediction': 1,
#     'probability': 0.08,
#     'credit_score': 625,
#     'risk_level': 'Medium',
#     'approval_status': 'conditional',
#     ...
# }
```

### SHAP API
```python
from backend.explainability.shap_explainer import initialize_shap_explainer

explainer, shap_values, X_explained = initialize_shap_explainer(model, X_train, "XGBoost")
importance_df = explainer.get_feature_importance()
local_exp = explainer.get_local_explanation(sample_idx=0, X=X_explained)
```

---

*Document Version: 1.0 | Last Updated: 2025-12-25*
